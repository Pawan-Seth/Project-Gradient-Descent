{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(X, Y, learning_rate, slope):\n",
    "    p = len(X[0])\n",
    "    q = len(X)\n",
    "    new_slope = np.zeros(p)\n",
    "    \n",
    "    for i in range(q):\n",
    "            x = X[i]\n",
    "            y = Y[i]\n",
    "            for j in range(p):\n",
    "                new_slope[j] += -(2/q) * (y - sum(slope*x)) * x[j]\n",
    "               \n",
    "    m = slope - (learning_rate * new_slope) \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, Y, slope):\n",
    "    total_cost = 0\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        total_cost += (y - sum(slope*x))**2\n",
    "    return total_cost/len(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(X, Y, learning_rate, num_iterations):\n",
    "    p = len(X[0])\n",
    "    slope = np.zeros(p)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        slope = step_gradient(X, Y, learning_rate, slope)\n",
    "        print(\"Cost: \",i, cost(X, Y, slope))\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"training_boston_x_y_train.csv\",delimiter = \",\")\n",
    "xt = data[:,:-1]\n",
    "y_train = data[:,-1]\n",
    "a = np.ones(379).reshape(-1,1)\n",
    "x_train = np.append(xt,a,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0 305.539675466323\n",
      "Cost:  1 165.2933425733978\n",
      "Cost:  2 97.06688188678649\n",
      "Cost:  3 63.21750497070385\n",
      "Cost:  4 46.030081681878364\n",
      "Cost:  5 37.04755970426135\n",
      "Cost:  6 32.17907582483823\n",
      "Cost:  7 29.41948101297267\n",
      "Cost:  8 27.771530562541034\n",
      "Cost:  9 26.730537113319105\n",
      "Cost:  10 26.03542463919629\n",
      "Cost:  11 25.54730098825756\n",
      "Cost:  12 25.18963319669134\n",
      "Cost:  13 24.918435676905123\n",
      "Cost:  14 24.70720553221768\n",
      "Cost:  15 24.539172286200976\n",
      "Cost:  16 24.40321656637249\n",
      "Cost:  17 24.291655752494144\n",
      "Cost:  18 24.198997657133127\n",
      "Cost:  19 24.121208614085\n",
      "Cost:  20 24.05526359128315\n",
      "Cost:  21 23.998856885025155\n",
      "Cost:  22 23.95020825609833\n",
      "Cost:  23 23.907928405084803\n",
      "Cost:  24 23.87092298140954\n",
      "Cost:  25 23.838322605708104\n",
      "Cost:  26 23.809431029058622\n",
      "Cost:  27 23.783686262272607\n",
      "Cost:  28 23.76063115982872\n",
      "Cost:  29 23.739890994270166\n",
      "Cost:  30 23.721156253478025\n",
      "Cost:  31 23.704169370928266\n",
      "Cost:  32 23.68871443574486\n",
      "Cost:  33 23.674609171722963\n",
      "Cost:  34 23.661698651653058\n",
      "Cost:  35 23.64985034423151\n",
      "Cost:  36 23.63895018841774\n",
      "Cost:  37 23.628899463227093\n",
      "Cost:  38 23.619612275994758\n",
      "Cost:  39 23.611013533717053\n",
      "Cost:  40 23.603037293565126\n",
      "Cost:  41 23.59562541256937\n",
      "Cost:  42 23.58872643466513\n",
      "Cost:  43 23.582294667166387\n",
      "Cost:  44 23.576289409343385\n",
      "Cost:  45 23.57067430391239\n",
      "Cost:  46 23.56541678849845\n",
      "Cost:  47 23.560487628950778\n",
      "Cost:  48 23.555860520121115\n",
      "Cost:  49 23.551511742609346\n",
      "Cost:  50 23.54741986623928\n",
      "Cost:  51 23.54356549279406\n",
      "Cost:  52 23.539931031932454\n",
      "Cost:  53 23.536500505305355\n",
      "Cost:  54 23.533259374767738\n",
      "Cost:  55 23.53019439127895\n",
      "Cost:  56 23.527293461647748\n",
      "Cost:  57 23.524545530732333\n",
      "Cost:  58 23.521940477077454\n",
      "Cost:  59 23.519469020273583\n",
      "Cost:  60 23.517122638574193\n",
      "Cost:  61 23.514893495515263\n",
      "Cost:  62 23.512774374454157\n",
      "Cost:  63 23.510758620091742\n",
      "Cost:  64 23.508840086164547\n",
      "Cost:  65 23.50701308859816\n",
      "Cost:  66 23.505272363503042\n",
      "Cost:  67 23.50361302946993\n",
      "Cost:  68 23.5020305536883\n",
      "Cost:  69 23.500520721468785\n",
      "Cost:  70 23.499079608799004\n",
      "Cost:  71 23.497703557606187\n",
      "Cost:  72 23.496389153437438\n",
      "Cost:  73 23.49513320530093\n",
      "Cost:  74 23.49393272744107\n",
      "Cost:  75 23.492784922844937\n",
      "Cost:  76 23.491687168301098\n",
      "Cost:  77 23.490637000850267\n",
      "Cost:  78 23.48963210548541\n",
      "Cost:  79 23.48867030397477\n",
      "Cost:  80 23.4877495446937\n",
      "Cost:  81 23.486867893364796\n",
      "Cost:  82 23.486023524615398\n",
      "Cost:  83 23.48521471427173\n",
      "Cost:  84 23.484439832317676\n",
      "Cost:  85 23.48369733645271\n",
      "Cost:  86 23.48298576619182\n",
      "Cost:  87 23.482303737454394\n",
      "Cost:  88 23.48164993759682\n",
      "Cost:  89 23.481023120845357\n",
      "Cost:  90 23.480422104093105\n",
      "Cost:  91 23.479845763026287\n",
      "Cost:  92 23.47929302855032\n",
      "Cost:  93 23.47876288348763\n",
      "Cost:  94 23.478254359523362\n",
      "Cost:  95 23.47776653437624\n",
      "Cost:  96 23.47729852917509\n",
      "Cost:  97 23.476849506022877\n",
      "Cost:  98 23.47641866573211\n",
      "Cost:  99 23.476005245716596\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.15\n",
    "itteration = 100\n",
    "m = gd(x_train,y_train,learning_rate,itteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(X,m):\n",
    "    N = len(X)\n",
    "    Y = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        Y[i] = (X[i]*m).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = np.loadtxt(\"test_boston_x_test.csv\",delimiter = \",\")\n",
    "test = np.ones(127).reshape(-1,1)\n",
    "test_data = np.append(testing_data,test,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Predict(test_data,m)\n",
    "np.savetxt(X=y_pred,fname='projectgradient3.csv' , delimiter=',', fmt='%.5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
